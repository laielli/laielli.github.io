name: "dynamic-programming"
flashcards:
  - question: Give a brief description of dynamic programming.
    answer: "Dynamic programming (DP) is an algorithmic technique used to efficiently solve problems with overlapping subproblems and optimal substructure. It works by breaking a problem down into smaller subproblems, solving each subproblem once, and storing their solutions (usually in a table or memo) so that each subproblem is only solved one time. By reusing these saved solutions, DP avoids recomputation and thus dramatically improves performance (often converting exponential brute-force solutions into polynomial-time solutions) for many optimization and combinatorial problems."
  - question: What is the primary benefit of using dynamic programming?
    answer: "Avoiding recomputation: The primary benefit of dynamic programming is that it eliminates duplicate calculations of subproblems. By storing and reusing results (memoization or tabulation), DP drastically reduces the time complexity compared to naive recursion. This means problems that would otherwise take exponential time can often be solved in polynomial time using DP, thanks to this reuse of intermediate results."
  - question: What is the typical time complexity of dynamic programming solutions?
    answer: "Typically polynomial: Most dynamic programming solutions run in polynomial time relative to the input size. The exact complexity depends on how many subproblems are defined and how hard each subproblem is to solve. Common DP algorithms are O(n), O(n^2), O(n*m), etc., rather than exponential. For example, computing the nth Fibonacci number via DP is O(n), and solving the knapsack problem with DP might be O(n*W) (where W is the capacity). The key is that each subproblem is solved once, leading to much lower time complexity than naive approaches."
  - question: What is the typical space complexity of dynamic programming solutions?
    answer: "O(n) or O(n*m): Dynamic programming usually requires extra memory to store subproblem results. Often the space complexity is on the same order as the number of subproblems. For instance, a one-dimensional DP (like Fibonacci with memoization) uses O(n) space, while a two-dimensional DP (like edit distance or a grid-based DP) uses O(n*m) space for an n by m table. In many cases, space can be optimized by only storing needed states (for example, using O(n) space instead of O(n*m) for some problems by reusing a single row or column of the table), but DP generally uses more memory than a greedy approach due to the caching of results."
  - question: When is dynamic programming most effective?
    answer: "Overlapping subproblems: Dynamic programming is most effective for problems that have overlapping subproblems and optimal substructure. This means the problem can be decomposed into smaller subproblems that repeat (overlap) many times, and the optimal solution of the full problem can be composed from the optimal solutions of the subproblems. When a naive recursion would recompute the same values repeatedly, DP shines by computing each needed value once. Classic scenarios include computing Fibonacci numbers, paths in grids (with overlapping paths), or many optimization problems (like knapsack or coin change) where sub-solutions are reused frequently."
  - question: What is a typical use case for dynamic programming?
    answer: "Optimization and path-finding problems: Dynamic programming is commonly used in scenarios such as:
    1. **Knapsack problem**: determining the optimal value achievable with given weight constraints using DP to consider subproblems of smaller capacities.
    2. **Sequence alignment or edit distance**: using DP to compute the minimum edit distance between two strings (as in Levenshtein distance) or to align DNA sequences.
    3. **Grid path problems**: counting or finding the shortest/longest path in a grid (unique paths, minimum path sum) where DP builds up solutions from smaller subgrids.
    4. **Partitioning problems**: like partitioning an array (palindrome partitioning, matrix chain multiplication), where DP finds optimal ways to break the problem into pieces.
    These use cases leverage DP to manage the exponential possibilities by solving sub-cases and reusing those results."
  - question: What is the typical implementation of dynamic programming?
    answer: "Dynamic programming is typically implemented in one of two ways:
    - **Top-down with memoization**: Write a recursive solution for the problem, then use a memo (cache) to store results of subproblems as you compute them. Before computing a subproblem, check the memo; if the result is already known, reuse it instead of recomputing.
    - **Bottom-up with tabulation**: Identify the subproblem states and their recurrence relation, then iteratively compute the results starting from base cases up to the final problem. This often involves filling out a table (array or matrix) where each cell represents a subproblemâ€™s solution, building up to the solution of the original problem.
    In both cases, implementing DP requires defining the state (what each subproblem represents), the recurrence (how to compute a state from smaller states), and using a storage structure (array, matrix, or dictionary) to save results."