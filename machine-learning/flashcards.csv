question,answer
"[Anistropic weight updates for 2D matrices] What is a 2D weight matrix with shape [out, in]?","A matrix W that maps an input vector x of length in to an output vector y of length out via y = W x."
"[Anistropic weight updates for 2D matrices] In y = W x, what are the shapes of x, y, and W?","x ∈ R^in, y ∈ R^out, W ∈ R^{out×in}."
"[Anistropic weight updates for 2D matrices] What is a weight update during training?","Applying a small step to weights: W <- W - η * Δ, where Δ is a (often momentum-smoothed) gradient-like update."
"[Anistropic weight updates for 2D matrices] What does 'dominated by a few directions' mean for a matrix update?","Most of the update’s effect is concentrated along a small number of input/output directions."
"[Anistropic weight updates for 2D matrices] Define anisotropy in this context.","Directional imbalance: the update stretches some directions much more than others."
"[Anistropic weight updates for 2D matrices] What decomposition reveals those 'directions'?","The Singular Value Decomposition (SVD): Δ = U Σ V^T."
"[Anistropic weight updates for 2D matrices] In the SVD, what are the right singular vectors (columns of V)?","Special input directions that Δ acts on."
"[Anistropic weight updates for 2D matrices] In the SVD, what are the left singular vectors (columns of U)?","Corresponding output directions after applying Δ."
"[Anistropic weight updates for 2D matrices] What do the singular values σ1 ≥ σ2 ≥ ... in Σ tell you?","How strongly each singular direction is scaled by the update."
"[Anistropic weight updates for 2D matrices] How do singular values indicate anisotropy?","If σ1 is much larger than the rest (only a few big σi), the update is highly anisotropic."
"[Anistropic weight updates for 2D matrices] What does 'low-rank-ish update' imply?","Only a few singular values are large, so the update effectively has low rank and focuses on few directions."
"[Anistropic weight updates for 2D matrices] Give the geometric mental picture for anisotropy.","Multiplying the unit circle (or sphere) by an anisotropic matrix turns it into a skinny ellipse—heavy stretch along 1–2 axes."
"[Anistropic weight updates for 2D matrices] In plain terms, what is the shape [out, in] telling you?","How many outputs the layer produces (out) from how many input features it receives (in)."
"[Anistropic weight updates for 2D matrices] Why do we care about anisotropy in updates?","Updates that favor only a few directions can neglect other useful directions, potentially slowing or biasing learning."
"[Constrain the operator norm] When bounding the induced change in outputs by constraining the operator norm of the inputs - what is a 'norm' in this context?","A norm is a size/length measure. For vectors, ‖x‖₂ is the Euclidean length; for matrices, there are several norms—here we care about an operator norm."
"[Constrain the operator norm] What does the operator (spectral) norm measure?","It measures how much a matrix A can stretch any vector: the maximum amplification factor over all unit vectors."
"[Constrain the operator norm] Give the formal definition of the operator norm.","‖A‖_{op} = max_{‖x‖₂ = 1} ‖A x‖₂."
"[Constrain the operator norm] How is the operator norm related to singular values?","For any matrix A, ‖A‖_{op} equals its largest singular value σ_max."
"[Constrain the operator norm] What does it mean to 'constrain the operator norm of the update'?","Limit ‖ΔW‖_{op} for the weight update ΔW so the update cannot stretch vectors beyond a chosen bound."
"[Constrain the operator norm] What is the relation between the update and the output change?","If you change weights by ΔW and input is x, then the output change is Δy = ΔW x."
"[Constrain the operator norm] State the inequality that bounds output change via the operator norm.","‖Δy‖₂ ≤ ‖ΔW‖_{op} · ‖x‖₂."
"[Constrain the operator norm] Why does constraining ‖ΔW‖_{op} bound output change for any input?","Because for all x, ‖ΔW x‖₂ ≤ ‖ΔW‖_{op}‖x‖₂; keeping ‖ΔW‖_{op} small guarantees small output changes regardless of x."
"[Constrain the operator norm] What practical benefit does this constraint provide during training?","Layer-wise stability control: it prevents large, destabilizing changes in a layer’s outputs from a single update."
"[Constrain the operator norm] Explain the 'worst-case stretch' intuition.","The operator norm is the worst-case stretch a matrix can apply; limiting it caps the maximum output change an update can cause."
"[Constrain the operator norm] How does this idea relate to preventing training 'thrash'?","By capping worst-case stretch (‖ΔW‖_{op}), you avoid sudden, extreme output shifts that can make training oscillate or diverge."
"[Constrain the operator norm] In one sentence, summarize the core idea of the slide.","Control the update’s operator norm to guarantee bounded output changes, yielding stable, predictable training steps."
"[Orthogonalizing the update] What does 'orthogonalizing the update' mean?","Replace uneven singular values with equal ones so the update has equal step size in every singular direction: ΔW ∝ U·I·Vᵀ = U·Vᵀ."
"[Orthogonalizing the update] Why does a norm constraint suggest orthogonalizing?","Under an operator-norm cap, the optimal loss-reducing update keeps U and V from the gradient but sets all singular values equal, distributing the step uniformly while saturating the constraint."
"[Orthogonalizing the update] What is the SVD-based form of the orthogonalized update?","ΔW ∝ U·I·Vᵀ, i.e., take the gradient’s singular vectors and replace Σ by a scaled identity."
"[Orthogonalizing the update] What does 'set all singular values equal' accomplish?","It removes the imbalance in Σ and spreads the step magnitude evenly across all singular directions."
"[Orthogonalizing the update] What is a 'semi-orthogonal' matrix U·Vᵀ?","A matrix with orthonormal columns/rows (depending on shape) whose action preserves lengths along its subspace; its spectral norm is 1."
"[Orthogonalizing the update] Give a plain-English analogy for orthogonalizing.","Flatten the volume knob: instead of a few 'loud' directions getting huge steps and 'quiet' ones tiny steps, every important direction gets the same step."
"[Orthogonalizing the update] How does orthogonalizing help training?","It combats few-directions dominance (directional imbalance), promoting more uniform (isotropic) feature learning."
"[Orthogonalizing the update] Which Transformer components benefit most from this?","The Value/Output (VO) projections and FFN layers, which act like associative memories where many directions matter."
"[Orthogonalizing the update] What issue does orthogonalizing directly address?","Low-rankish, anisotropic updates where most of the step is concentrated in only a few directions."
"[Orthogonalizing the update] What are 'singular directions' in this context?","The left/right singular vectors (U, V) from the SVD; they are the special input/output directions the update acts on."
"[Orthogonalizing the update] What does a cap on ‖ΔW‖_{op} imply about the optimal update?","A trust-region style solution: use the full norm budget and allocate it evenly across the singular directions (ΔW ∝ U·I·Vᵀ)."
"[Orthogonalizing the update] Summarize the core idea in one sentence.","Constrain the update’s operator norm and orthogonalize it so every singular direction gets the same step size, preventing dominance by a few directions."
"[RMS norm] What is the RMS norm of a d-dimensional vector x?","The RMS (root-mean-square) norm is defined as ‖x‖_{RMS} = sqrt((1/d) * Σ_i x_i^2)."
"[RMS norm] How is the RMS norm related to the usual Euclidean (ℓ2) norm for a vector x ∈ ℝ^d?","They are proportional: ‖x‖_{RMS} = ‖x‖_2 / sqrt(d)."
"[RMS norm] Why use the RMS norm instead of the Euclidean norm when comparing changes across layers of different widths?","RMS normalizes by sqrt(d), so typical magnitudes are comparable even if you change the layer width; this makes 'how big is a change?' more consistent across layers."
"[RMS norm] In plain words, what does the RMS norm measure?","It measures the average magnitude per component of a vector (the Euclidean length scaled by 1/√d)."
"[RMS norm] If you know ‖x‖_2 and the dimension d, how can you quickly compute ‖x‖_{RMS}?","Use ‖x‖_{RMS} = ‖x‖_2 / sqrt(d)."
"[RMS norm] Numerical check: For x = (3, 4) in ℝ^2, what is ‖x‖_{RMS}?","‖x‖_2 = 5, d = 2, so ‖x‖_{RMS} = 5 / √2 ≈ 3.536."
"[RMS norm] How does doubling the width d of a layer affect typical RMS sizes of vectors passing through it?","RMS stays on a comparable scale because it divides by √d; doubling d does not double the RMS magnitude."
"[RMS norm] What benefit does RMS normalization provide when reasoning about per-layer output changes?","It gives a dimension-agnostic scale, so bounds and comparisons on changes remain meaningful across layers of different sizes."
"[RMS→RMS operator norm of a matrix W] What is the RMS→RMS operator norm of a matrix W (mapping ℝ^{in}→ℝ^{out})?","It is defined as ‖W‖_{RMS→RMS} = sup_{x ≠ 0} ( ‖W x‖_{RMS} / ‖x‖_{RMS} ), i.e., the largest factor by which W can scale a vector's RMS size."
"[RMS→RMS operator norm of a matrix W] What does 'sup_{x ≠ 0}' mean in the definition of ‖W‖_{RMS→RMS}?","It means we consider all nonzero input vectors x and take the supremum (least upper bound) of the ratio ‖W x‖_{RMS}/‖x‖_{RMS}; practically, the worst-case stretch."
"[RMS→RMS operator norm of a matrix W] In plain words, what does the RMS→RMS operator norm measure for W?","The maximum possible amplification of RMS magnitude that W can apply to any input vector."
"[RMS→RMS operator norm of a matrix W] How is the RMS→RMS operator norm related to the usual (Euclidean) spectral norm?","They measure the same 'largest stretch' idea; under RMS norms the value equals a constant times the spectral norm."
"[RMS→RMS operator norm of a matrix W] Give the formula linking the RMS→RMS operator norm to the largest singular value σ_max(W).","‖W‖_{RMS→RMS} = sqrt(in/out) · σ_max(W), where 'in' and 'out' are W's input and output dimensions."
"[RMS→RMS operator norm of a matrix W] What is σ_max(W) in this context?","It is the largest singular value of W (the spectral norm under Euclidean norms), representing W's maximum stretch factor on unit-length inputs."
"[RMS→RMS operator norm of a matrix W] Why use RMS→RMS instead of the Euclidean operator norm in deep nets?","RMS normalizes by dimension, making 'how big is a change?' comparable across layers of different widths."
"[RMS→RMS operator norm of a matrix W] If ‖W‖_{RMS→RMS} is large, what does that imply about W's action on inputs?","There exists some input direction for which W can increase RMS magnitude by a large factor (i.e., a strong worst-case stretch)."
"[RMS→RMS operator norm of a matrix W] What happens to ‖W‖_{RMS→RMS} if we widen the layer (increase 'out') with similar singular values?","Because of the sqrt(in/out) factor, increasing 'out' decreases ‖W‖_{RMS→RMS} proportionally (all else equal), reflecting RMS normalization."
"[RMS→RMS operator norm of a matrix W] How can the RMS→RMS operator norm be used during training?","By constraining or penalizing ‖ΔW‖_{RMS→RMS}, we bound the worst-case RMS change in a layer’s output per update, improving stability."
"[RMS→RMS operator norm of a matrix W] State the output-change inequality that follows from the operator norm definition.","For an update ΔW and input x: ‖Δy‖_{RMS} = ‖ΔW x‖_{RMS} ≤ ‖ΔW‖_{RMS→RMS} · ‖x‖_{RMS}."
"[RMS→RMS operator norm of a matrix W] What intuition should I remember about ‖W‖_{RMS→RMS}?","Think 'worst-case stretch' of RMS size: the most W can inflate an input’s RMS magnitude, normalized for input/output dimensions."
"[Perturbation bound] What is the perturbation bound for a linear layer update in RMS norms?","If weights change by ΔW and input is x, then Δy = ΔW x and ‖Δy‖_{RMS} ≤ ‖ΔW‖_{RMS→RMS} · ‖x‖_{RMS}."
"[Perturbation bound] In the bound Δy = ΔW x, what do Δy and ΔW represent?","ΔW is the weight update (the change applied to the weight matrix), and Δy is the resulting change in the layer's output for input x."
"[Perturbation bound] What norms are used in the perturbation bound?","Both input and output use the RMS norm (‖·‖_{RMS}); the matrix size is measured by the RMS→RMS operator norm ‖ΔW‖_{RMS→RMS}."
"[Perturbation bound] Why is the inequality ‖Δy‖_{RMS} ≤ ‖ΔW‖_{RMS→RMS} · ‖x‖_{RMS} true?","It follows directly from the definition of an operator norm: for any matrix A and vector x, ‖A x‖ ≤ ‖A‖ · ‖x‖ in the corresponding norms; here A = ΔW and both norms are RMS."
"[Perturbation bound] What does ‖ΔW‖_{RMS→RMS} intuitively measure?","The worst-case 'stretch' that the update ΔW can apply to the RMS size of any input vector."
"[Perturbation bound] What guarantee do we get if we cap ‖ΔW‖_{RMS→RMS}?","No input can cause the layer’s output RMS to change by more than that cap times the input’s RMS; this yields a clean per-layer stability guarantee."
"[Perturbation bound] Does the perturbation bound depend on the direction of x?","No. The operator norm is a worst-case over all directions, so the bound holds for any input x."
"[Perturbation bound] Give a numeric example using the bound.","If ‖ΔW‖_{RMS→RMS} = 0.05 and ‖x‖_{RMS} = 2, then ‖Δy‖_{RMS} ≤ 0.05 × 2 = 0.1."
"[Perturbation bound] How does this relate to 'trust region' intuition in optimization?","Capping ‖ΔW‖_{RMS→RMS} acts like a layer-wise trust region, limiting the maximum output change per update step."
"[Perturbation bound] What is the role of the RMS→RMS operator norm specifically?","It bridges matrix and vector sizes under RMS scaling, ensuring ‖ΔW x‖_{RMS} cannot exceed ‖ΔW‖_{RMS→RMS} times ‖x‖_{RMS}."
"[Perturbation bound] Restate the perturbation bound in plain English.","The biggest output change you can get from one update is at most 'how large the update can stretch things' times 'how big the input is' (all measured with RMS)."
"[Perturbation bound] Why is using RMS important in this context?","RMS normalizes by dimension, making bounds comparable across layers of different widths while still providing a tight worst-case guarantee."
"[Why controlling ‖ΔW‖_{RMS→RMS} matters] In neural network training, why can large, sudden output changes after a weight update be harmful?","They can cause 'thrashing'—oscillatory or unstable behavior—and even divergence, because the model’s intermediate representations shift too far between steps."
"[Why controlling ‖ΔW‖_{RMS→RMS} matters] What quantity can we control to limit worst-case output change from a single layer update?","The RMS→RMS operator norm of the update matrix, ‖ΔW‖_{RMS→RMS}."
"[Why controlling ‖ΔW‖_{RMS→RMS} matters] How does capping ‖ΔW‖_{RMS→RMS} improve training stability?","It bounds the worst-case RMS size of the output change per step, acting like a built-in, layer-wise trust region."
"[Why controlling ‖ΔW‖_{RMS→RMS} matters] What is the 'trust region' intuition behind controlling ‖ΔW‖_{RMS→RMS}?","You limit how far the layer’s function can move in one step, preventing overly aggressive updates that destabilize training."
"[Why controlling ‖ΔW‖_{RMS→RMS} matters] State the perturbation inequality that links the update size to the output change (in RMS terms).","‖Δy‖_{RMS} ≤ ‖ΔW‖_{RMS→RMS} · ‖x‖_{RMS} for any input x."
"[Why controlling ‖ΔW‖_{RMS→RMS} matters] If ‖ΔW‖_{RMS→RMS} = 0.05 and ‖x‖_{RMS} = 2, what is the bound on the output change?","‖Δy‖_{RMS} ≤ 0.05 × 2 = 0.1."
"[Why controlling ‖ΔW‖_{RMS→RMS} matters] What does the numeric bound ‖Δy‖_{RMS} ≤ 0.1 mean in practice?","No matter the input direction, a single update cannot change that layer’s output RMS by more than 0.1, yielding predictable, bounded behavior."
"[Why controlling ‖ΔW‖_{RMS→RMS} matters] Does the bound ‖Δy‖_{RMS} ≤ ‖ΔW‖_{RMS→RMS} · ‖x‖_{RMS} depend on the direction of x?","No; it is a worst-case guarantee that holds for any input direction."
"[Why controlling ‖ΔW‖_{RMS→RMS} matters] How does controlling ‖ΔW‖_{RMS→RMS} relate to per-layer vs whole-network stability?","It provides a local (per-layer) guarantee; stacking such guarantees across layers contributes to overall network stability."
"[Why controlling ‖ΔW‖_{RMS→RMS} matters] Why is the RMS normalization useful when applying these bounds across layers of different widths?","RMS scales by 1/√d, making magnitudes comparable across layers with different dimensions, so the stability bound is dimension-agnostic."
"[Why controlling ‖ΔW‖_{RMS→RMS} matters] Summarize why controlling ‖ΔW‖_{RMS→RMS} matters for training in one sentence.","It prevents large, destabilizing output jumps by capping the worst-case effect of each update, improving stability and predictability of learning."
"[What the optimization problem is saying] What is the constrained optimization problem stated on the slide?","Minimize the first-order loss decrease subject to an operator-norm cap:  min_{ΔW} ⟨∇_W L, ΔW⟩  s.t.  ‖ΔW‖_{RMS→RMS} ≤ η."
"[What the optimization problem is saying] What does ∇_W L represent in this context?","It is the matrix gradient of the loss with respect to the weight matrix W (same shape as W)."
"[What the optimization problem is saying] What is the Frobenius inner product ⟨A,B⟩ between two matrices A and B?","⟨A,B⟩ = trace(A^T B) = Σ_{i,j} A_{ij} B_{ij}."
"[What the optimization problem is saying] What does minimizing ⟨∇_W L, ΔW⟩ achieve?","It chooses an update ΔW that points as opposite to the gradient as possible, giving the steepest first-order loss decrease."
"[What the optimization problem is saying] What does the constraint ‖ΔW‖_{RMS→RMS} ≤ η enforce on the update?","It limits the update size in a matrix-aware norm (RMS→RMS operator norm), capping the worst-case output stretch the update can cause."
"[What the optimization problem is saying] What does the RMS→RMS operator norm intuitively measure for ΔW?","The maximum factor by which ΔW can increase the RMS size of any input (a worst-case output change per unit input)."
"[What the optimization problem is saying] What is the 'trust region' analogy for this constraint?","The norm cap acts like a per-layer trust region radius η, preventing overly large, destabilizing updates in one step."
"[What the optimization problem is saying] In the analogous vector problem with an ℓ2 constraint, what is the solution?","Step in the negative gradient direction with length η: Δw = -η ∇_w L / ‖∇_w L‖_2 (or simply -η ∇_w L if the constraint is written that way)."
"[What the optimization problem is saying] Under a matrix operator-norm constraint, what plays the role of 'direction' and 'length'?","The singular vectors (U and V) give the direction; the singular values determine the per-direction step magnitudes."
"[What the optimization problem is saying] What does the parameter η represent in the constraint ‖ΔW‖_{RMS→RMS} ≤ η?","A step-size budget or radius: the maximum allowed worst-case stretch of the layer’s output induced by the update."
"[What the optimization problem is saying] What happens if we remove the constraint and only minimize ⟨∇_W L, ΔW⟩?","The problem is unbounded below; the best decrease pushes ΔW to infinite magnitude along the negative gradient—hence a constraint (or regularization) is necessary."
"[What the optimization problem is saying] Why is this called 'dualize the gradient under a matrix norm'?","Because we solve a steepest-descent problem where the step is measured in the dual (operator) norm, yielding an update aligned with the gradient’s singular directions subject to that norm budget."
"[What the optimization problem is saying] How does using RMS→RMS (instead of plain Euclidean) help across layers?","RMS normalization makes magnitudes comparable across different widths, so the same η has a consistent meaning across layers."
"[What the optimization problem is saying] In one sentence, summarize the slide's core idea.","Pick the update that most opposes the gradient while staying within an operator-norm trust region, which controls worst-case output change at the layer."
"[SVD background] What is the Singular Value Decomposition (SVD) of a matrix G?","It factors G into three matrices: G = U Σ V^T, where U and V have orthonormal columns and Σ is diagonal with nonnegative singular values."
"[SVD background] In the SVD G = U Σ V^T, what are the shapes of U, Σ, and V for G ∈ ℝ^{out×in}?","U ∈ ℝ^{out×r}, Σ ∈ ℝ^{r×r}, V ∈ ℝ^{in×r}, where r = rank(G)."
"[SVD background] What does it mean that U and V have 'orthonormal columns' in the SVD?","Each column is unit length and columns are mutually orthogonal; i.e., U^T U = I_r and V^T V = I_r."
"[SVD background] What are the singular values in Σ, and how are they ordered?","The diagonal entries σ_1 ≥ σ_2 ≥ … ≥ σ_r ≥ 0 are the singular values of G, sorted in nonincreasing order."
"[SVD background] What is the rank parameter r in the SVD?","r is the rank of G (the number of nonzero singular values), and it determines the number of singular vector pairs kept."
"[SVD background] What is the operator (spectral) norm of G in terms of its SVD?","‖G‖_{op} = σ_max(G) = σ_1, the largest singular value."
"[SVD background] Give the geometric intuition for U, Σ, and V in G = U Σ V^T.","V provides input directions, U provides the corresponding output directions, and Σ scales each direction by its singular value."
"[SVD background] How does G act on a right singular vector v_i from V?","G v_i = σ_i u_i, i.e., it maps the input direction v_i to the output direction u_i scaled by σ_i."
"[SVD background] What is the meaning of 'left' and 'right' singular vectors in SVD?","Left singular vectors are the columns of U (output-space directions), and right singular vectors are the columns of V (input-space directions)."
"[SVD background] Why are singular values always nonnegative?","They are defined as square roots of eigenvalues of G^T G (or G G^T), which are nonnegative by construction."
"[SVD background] How does the SVD relate to the 'largest stretch' a matrix can apply to a vector?","The largest stretch is the operator norm ‖G‖_{op} = σ_1; it is the maximum factor by which G can amplify a unit vector."
"[SVD background] If G is low-rank, what does its SVD look like?","Only a few singular values in Σ are nonzero (or large), so G ≈ U_r Σ_r V_r^T where r is small."
"[SVD background] How can SVD help interpret anisotropy (directional imbalance) of a matrix update?","Large gaps among singular values mean G stretches some directions (v_i) much more than others, indicating directional imbalance."
"[SVD background] In practical terms, what information do U, Σ, and V give you for analyzing a layer’s weight update matrix G?","V tells which input directions are being updated, U tells to which output directions they map, and Σ tells the per-direction update magnitudes."
"[Why the solution is ΔW* ∝ U Vᵀ] What optimization problem are we solving to choose the matrix update ΔW?","We minimize the first-order loss decrease subject to an operator-norm cap:  min_{ΔW} ⟨G, ΔW⟩  s.t.  ‖ΔW‖_{op} ≤ η, where G = ∇_W L and ‖·‖_{op} is the (RMS→RMS-equivalent) operator norm."
"[Why the solution is ΔW* ∝ U Vᵀ] What does it mean to 'make ⟨G, ΔW⟩ as negative as possible' in this matrix setting?","It means choosing ΔW that most strongly opposes the gradient G in Frobenius inner-product sense (steepest descent under the given norm budget)."
"[Why the solution is ΔW* ∝ U Vᵀ] What are the 'dual norms for matrices' used in this result?","The operator (spectral) norm ‖·‖_{op} is dual to the nuclear norm ‖·‖_* (sum of singular values):  max_{‖ΔW‖_{op} ≤ η} -⟨G,ΔW⟩ = η‖G‖_*."
"[Why the solution is ΔW* ∝ U Vᵀ] What is the nuclear norm ‖G‖_* and why does it appear here?","‖G‖_* is the sum of G's singular values. It appears via norm duality: maximizing linear decrease under an operator-norm constraint yields a value proportional to the nuclear norm of G."
"[Why the solution is ΔW* ∝ U Vᵀ] How does the duality result guide the form of the optimal ΔW?","It says the maximizing (most negative inner-product) ΔW should align with G's singular vector directions while respecting the operator-norm budget."
"[Why the solution is ΔW* ∝ U Vᵀ] What alignment with 'singular directions' means in practice?","If G = UΣVᵀ (SVD), alignment means ΔW shares the same left/right singular vectors U and V as G."
"[Why the solution is ΔW* ∝ U Vᵀ] What update matrix achieves the optimum under the operator-norm cap?","ΔW* = -η U Vᵀ, i.e., keep U and V from G's SVD but replace Σ with the identity (scaled to saturate the norm budget)."
"[Why the solution is ΔW* ∝ U Vᵀ] What does 'orthogonalized' update U Vᵀ mean geometrically?","U Vᵀ is (semi-)orthogonal: it preserves lengths along its subspace and has operator norm 1, so scaling by η gives ‖ΔW*‖_{op} = η."
"[Why the solution is ΔW* ∝ U Vᵀ] Why do we 'normalize the singular values' (set them all equal) in ΔW*?","Equalizing them removes directional imbalance, distributing the step evenly across all singular directions identified by the gradient."
"[Why the solution is ΔW* ∝ U Vᵀ] How does ΔW* relate to a trust-region viewpoint?","ΔW* saturates the trust region: its operator norm equals the radius η, using the full allowable step while keeping worst-case output change bounded."
"[Why the solution is ΔW* ∝ U Vᵀ] What problem does this 'orthogonalized' step help mitigate compared to a raw gradient step?","It combats rank collapse / directional dominance, where most of the update is concentrated in a few directions, by spreading the step equally across singular directions."
"[Why the solution is ΔW* ∝ U Vᵀ] State the TL;DR for why ΔW* ∝ U Vᵀ is optimal under the operator-norm cap.","Under an operator-norm constraint, the steepest descent step is the orthogonalized gradient: keep the gradient’s singular vectors (U,V) but set all singular values equal, yielding ΔW* ∝ U Vᵀ."
"[Why the solution is ΔW* ∝ U Vᵀ] If G = UΣVᵀ with singular values (σ₁,…,σ_r), how do the singular values of ΔW* compare?","They are all equal in magnitude (after scaling by η), unlike G’s possibly uneven σ_i; this enforces equal step size per singular direction."
"[Why the solution is ΔW* ∝ U Vᵀ] Why doesn't ΔW* copy the magnitudes in Σ from the gradient G?","Because the operator-norm budget caps the largest singular value; distributing that budget uniformly across directions maximizes the inner-product decrease under the constraint."
"[Orthogonalized update - practical meaning] What does an 'orthogonalized update' mean for a matrix gradient step?","Take the gradient’s SVD directions (U and V) and set all singular values equal; the update is ΔW ∝ U·V^T, which is (semi-)orthogonal."
"[Orthogonalized update - practical meaning] What is meant by '(semi-)orthogonal' for U·V^T?","Depending on shape, U·V^T has orthonormal columns or rows; it preserves lengths on its subspace and has operator norm 1."
"[Orthogonalized update - practical meaning] What is the operator (spectral) norm of the orthogonalized matrix U·V^T?","It is 1; thus scaling by η gives an update with operator norm η that saturates the trust region."
"[Orthogonalized update - practical meaning] Why equalize singular values in an orthogonalized update?","To prevent a few large singular values from hogging the step; every singular direction receives the same step magnitude (balanced update)."
"[Orthogonalized update - practical meaning] How does an orthogonalized update combat 'rank collapse' or directional imbalance?","By spreading the step evenly across all singular directions identified by the gradient instead of pushing almost only along the top singular modes."
"[Orthogonalized update - practical meaning] What practical behavior does Muon enforce regarding update directions?","Muon enforces orthogonalized updates for 2D weight matrices: keep U,V and flatten singular values so each direction gets an equal step."
"[Orthogonalized update - practical meaning] In a numeric example with gradient singular values [5, 2, 0.5], what happens with a plain (unconstrained) step?","The step is effectively weighted by [5, 2, 0.5], so the top direction dominates and the update is highly unbalanced."
"[Orthogonalized update - practical meaning] In the same example, what happens with an orthogonalized step before applying the norm budget η?","The singular values are set to [1, 1, 1], giving equal step size in each singular direction; then the whole matrix is scaled to meet the operator-norm budget η."
"[Orthogonalized update - practical meaning] How does scaling by η relate to an operator-norm trust region?","We choose ΔW* = -η·U·V^T so that ‖ΔW*‖_{op} = η, using the full allowed worst-case stretch while keeping changes bounded."
"[Orthogonalized update - practical meaning] What is the one-sentence takeaway of the orthogonalized-step idea?","Under an operator-norm trust region, the steepest loss-reducing step is the orthogonalized gradient: keep U,V and set all singular values equal (ΔW* ∝ U·V^T)."
"[Orthogonalized update - practical meaning] Why is 'operator norm = 1' for U·V^T a useful property?","It makes the step’s worst-case stretch predictable; multiplying by η directly sets the maximum output change per unit input in RMS terms."
"[Orthogonalized update - practical meaning] How does orthogonalizing affect learning rare or diverse features compared to a raw gradient step?","It promotes isotropic (even) progress across directions, helping capture features that would be under-updated when the raw gradient is dominated by a few directions."